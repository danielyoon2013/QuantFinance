{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes from default:\n",
    "\n",
    "## Effect of changes:\n",
    "N/A\n",
    "\n",
    "## 0. Overview\n",
    "\n",
    "\"Quantifying unstructured and noisy textual content is complex and involves numerous methodological issues related to the preprocessing of the data and the optimization of the algorithm used to quantify textual content. The number of **text preprocessing** that can be implemented is numerous (**lowercase**, **stemming**, **lemma-tization**, part-of-speech tagging, **stopwords removal**, **punctuation removal**, etc.) and it is not easy to identify which transformation increases (decreases) the accuracy of the classiﬁcation. The same is true for the choice of the algorithm: the large number of algorithms (Naive Bayes, SVM, logistic regression, random forest, multilayer perceptron, etc.) and the even greater number of hyperparameters for each algorithm lead to an immense number of combinations.Furthermore, the answers relative to those methodological issues strongly depend on the type of data used (informal or formal content, short or long text), on the size of the dataset (few hundreds or mil-lions of documents), on the availability of pre-classiﬁed messages (supervised or unsupervised learning), and on the type of documents (domain-speciﬁc or generic documents). While there is no one-ﬁts-all solution, we nonetheless believe that some guidance and tips can help researchers to avoid common mistakes.\"\n",
    "\n",
    "Renault, Thomas. (2020). Sentiment analysis and machine learning in finance: a comparison of methods and models on one million messages. Digital Finance. 2. 10.1007/s42521-019-00014-x. pp. 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('all-corpora')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data\n",
    "Current status: Default (~5000 tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this in testing.\n",
    "data = pd.read_csv('stock_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Clean and Lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_lower(text):\n",
    "    '''\n",
    "    Input:\n",
    "        A string of messy data\n",
    "    Output:\n",
    "        A string of clean lowercase data\n",
    "    \n",
    "    1) Replacing everything that isn't a letter with a space (special characters, numbers)\n",
    "    2) Sending all text to lowercase\n",
    "    '''\n",
    "    clean_text = re.sub('[^a-zA-Z]',\" \", text)\n",
    "    clean_text = clean_text.lower()\n",
    "    return clean_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Tokenize\n",
    "Current status: Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(text):\n",
    "    '''\n",
    "    Input:\n",
    "        A string of cleaned data\n",
    "    Output:\n",
    "        A list of the words in the string\n",
    "        \n",
    "    Will split the string into tokens based on what nltk package thinks is best.\n",
    "    Not sure how different it is compared to string.split(' ')\n",
    "    \n",
    "    There is also a TwitterTokenizer, maybe something to look into if we want to deal with \n",
    "        emojiis instead of replacing them with spaces.\n",
    "    '''\n",
    "    return word_tokenize(text, language='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Lemmatize and Stopword removal\n",
    "Current status: Lemmatization on\n",
    "\n",
    "Stopword removal on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_and_stopwords(text, remove_stop_words=False):\n",
    "    '''\n",
    "    Input:\n",
    "        List of words that have been cleaned and tokenized\n",
    "    Output:\n",
    "        A string that is supposed to represent the meaning of the original sentence, as reduced as possible\n",
    "        \n",
    "        \n",
    "    From from https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "    \"\n",
    "        Lemmatization is the process of converting a word to its base form. The difference \n",
    "            between stemming and lemmatization is, lemmatization considers the context and \n",
    "            converts the word to its meaningful base form, whereas stemming just removes the \n",
    "            last few characters, often leading to incorrect meanings and spelling errors.\n",
    "\n",
    "        For example, lemmatization would correctly identify the base form of ‘caring’ to ‘care’, \n",
    "            whereas, stemming would cutoff the ‘ing’ part and convert it to car.\n",
    "\n",
    "        ‘Caring’ -> Lemmatization -> ‘Care’\n",
    "        ‘Caring’ -> Stemming -> ‘Car’\n",
    "    \"\n",
    "    \n",
    "    \n",
    "    Since the original paper also mentioned stop words, I made sure the word is not a stop word\n",
    "    before trying to convert it. I ran it with and without the stop word and I didn't notice much difference\n",
    "    but maybe it's something to fiddle with later.\n",
    "    \n",
    "    \n",
    "    EDIT 1: The paper also mentioned 'part-of-speech' tagging, which seems to attach context to each word to help\n",
    "    convert its meaning properly. Since this seemed more complicated, and this isn't a text processing project,\n",
    "    I didn't do it. \n",
    "    However there is an entire coded example (example 3) here: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "    so it might be quick to implement and see if it improves our classification scores after\n",
    "    \n",
    "    EDIT 2:\n",
    "    About stop words, from the paper: \n",
    "        We also ﬁnd that removing stopwords using the NLTK stopwords corpus signiﬁcantly decreases the accuracy \n",
    "        of the classiﬁcation. We believe that this result is due to the fact that the stopwords corpus from NLTK includes \n",
    "        words that could be very useful for sentiment analysis in ﬁnance such as “up”, “down”, “below” or “above”. Thus, \n",
    "        researchers should not use the standard NLTK list and should consider a more restrictive list of stopwords for \n",
    "        sentiment analysis (“a”, “an”, “the”...). This result is consistent with Saif et al (2014) who show that Naive \n",
    "        Bayes classiﬁers are more sensitive to stopword removal and that using pre-existing lists of stopwords negatively \n",
    "        impacts the performance of sentiment classiﬁcation for short-messages posted on social media.\n",
    "    '''\n",
    "    if remove_stop_words:\n",
    "        list_of_words = [lemmatizer.lemmatize(word) for word in text if(word) not in stop_words_set]\n",
    "    else:\n",
    "        list_of_words = [lemmatizer.lemmatize(word) for word in text]\n",
    "        \n",
    "    string_of_words = \" \".join(list_of_words)\n",
    "    return string_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_all_data(data, remove_stop_words=False):\n",
    "    '''\n",
    "    Input:\n",
    "        The column of the dataframe that contains text\n",
    "    Output:\n",
    "        A list of strings that has been through:\n",
    "            clean_data = clean_and_lower(data)\n",
    "            token_data = tokenize_data(clean_data)\n",
    "            lemma_data = lemma_and_stopwords(token_data)\n",
    "    '''\n",
    "    list_of_nice_strings = []\n",
    "    for i in data:\n",
    "        clean_string = clean_and_lower(i)\n",
    "        token_list = tokenize_data(clean_string)\n",
    "        lemma_string = lemma_and_stopwords(token_list, remove_stop_words=remove_stop_words)\n",
    "        list_of_nice_strings.append(lemma_string)\n",
    "        \n",
    "    return list_of_nice_strings\n",
    "\n",
    "# Note: Just eyeballing it, it looks like some words that are useful are being thrown out\n",
    "# I.e in fourth row (index 3) it goes from \"MNTA Over 12\" --> \"mnta\". It seems like that is a bullish tweet, but something\n",
    "# is throwing away the \"over\".\n",
    "# EDIT: I just tried without \"stop words\" and it recovered the \"Over\"... Best to try both\n",
    "\n",
    "clean_data = prep_all_data(data['Text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Vectorization\n",
    "Current status:\n",
    "\n",
    "Vectorization: CountVectorizer\n",
    "\n",
    "n-grams: 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_num(clean_data):\n",
    "    '''\n",
    "    Input: \n",
    "        cleaned list of strings\n",
    "    Output:\n",
    "        Numerical vector representation [0's and 1's]\n",
    "    \n",
    "    From: https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "    More info: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "    \n",
    "    Text Analysis is a major application field for machine learning algorithms. However,\n",
    "        the raw data, a sequence of symbols cannot be fed directly to the algorithms \n",
    "        themselves as most of them expect numerical feature vectors with a fixed size \n",
    "        rather than the raw text documents with variable length.\n",
    "    ...       \n",
    "    We call vectorization the general process of turning a collection of text documents into numerical \n",
    "        feature vectors. This specific strategy (tokenization, counting and normalization) is called \n",
    "        the Bag of Words or “Bag of n-grams” representation. Documents are described by word occurrences \n",
    "        while completely ignoring the relative position information of the words in the document.\n",
    "    '''\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    # Like usual we may need to cross validate this to determine the optimal represenation\n",
    "    # Cross validation link: https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py\n",
    "    # max_features = ?\n",
    "    # ngram_range = ?\n",
    "    # fit_transform: Learn the vocabulary dictionary and return document-term matrix.\n",
    "    # EDIT: He has results in his paper about choosing good parameters for ngrams\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 2), min_df=3)\n",
    "    document_term_matrix = vectorizer.fit_transform(clean_data).toarray()\n",
    "    return document_term_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dimension reduction\n",
    "Current status:\n",
    "\n",
    "No dimension reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# None by default.\n",
    "# When we add this, we'll do it off the X matrix here.\n",
    "\n",
    "X = text_to_num(clean_data)\n",
    "y = data[\"Sentiment\"]\n",
    "y = y.replace(-1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training/Testing split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (5791, 6298)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print(\"X Shape: {}\".format(X.shape))\n",
    "# Should we do PCA on this? X is (5791, 8330)....\n",
    "# from sklearn.decomposition import PCA\n",
    "# n_components=250\n",
    "# pca = PCA(n_components=n_components) \n",
    "# X_reduced = pca.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2021)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model list definition:\n",
    "Current status: Multinomial NB, Logistic Regression, SVM, Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "models_to_try = []\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "models_to_try.append((\"MultinomialNB\",MultinomialNB()))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "# Fitting gaussing naive bayes (like in class) instead\n",
    "# models_to_try.append((\"GaussianNB\", GaussianNB()))\n",
    "\n",
    "# Logistic Regression\n",
    "models_to_try.append((\"LogisticReg\", LogisticRegression()))\n",
    "\n",
    "# Support Vector Machine\n",
    "models_to_try.append((\"SVC\", SVC()))\n",
    "# models_to_try.append((\"SVC\", SVC(probability=True))) # Need this to use .predict_proba()\n",
    "\n",
    "# Random Forest\n",
    "models_to_try.append((\"RandomForest\", RandomForestClassifier()))\n",
    "\n",
    "# Multilayer Perceptron \n",
    "# Not going to fit a multilayer perceptron, here's XGB instead\n",
    "#models_to_try.append((\"XGB\", XGBClassifier()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Accuracy: 0.7894736842105263\n",
      "MultinomialNB AUC: 0.8578243175797596\n",
      "MultinomialNB MCC: 0.5513620600876112\n",
      "LogisticReg Accuracy: 0.8024158757549612\n",
      "LogisticReg AUC: 0.8668208750847827\n",
      "LogisticReg MCC: 0.5684756323345909\n",
      "SVC Accuracy: 0.7842968075927523\n",
      "SVC MCC: 0.5224622864520863\n",
      "RandomForest Accuracy: 0.7946505608283002\n",
      "RandomForest AUC: 0.8609692734927887\n",
      "RandomForest MCC: 0.5570758675771761\n",
      "MultinomialNB Confusion Matrix\n",
      "           True Y=1  True Y=0\n",
      "Guess Y=1       602       114\n",
      "Guess Y=0       130       313\n",
      "\n",
      "\n",
      "LogisticReg Confusion Matrix\n",
      "           True Y=1  True Y=0\n",
      "Guess Y=1       638       135\n",
      "Guess Y=0        94       292\n",
      "\n",
      "\n",
      "SVC Confusion Matrix\n",
      "           True Y=1  True Y=0\n",
      "Guess Y=1       663       181\n",
      "Guess Y=0        69       246\n",
      "\n",
      "\n",
      "RandomForest Confusion Matrix\n",
      "           True Y=1  True Y=0\n",
      "Guess Y=1       617       123\n",
      "Guess Y=0       115       304\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import  roc_auc_score\n",
    "def prediction_results(models_to_try, X_train, X_test, y_train, y_test):\n",
    "    '''\n",
    "    Input:\n",
    "        models_to_try: a list of tuples (\"Name as a string\", object)\n",
    "        Data as usual\n",
    "    Ouput:\n",
    "        Prints the accuracy and returns a dictionary to create a confusion matrix\n",
    "    \n",
    "    '''\n",
    "    confused_dict=dict()\n",
    "    for name, classifier in models_to_try:\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "            \n",
    "        preds = classifier.predict(X_test)\n",
    "        print(\"{} Accuracy: {}\".format(name, accuracy_score(y_test, preds)))\n",
    "        \n",
    "        if name != \"SVC\":\n",
    "            probs = classifier.predict_proba(X_test)[:, 1]\n",
    "            print(\"{} AUC: {}\".format(name, roc_auc_score(y_test, probs)))\n",
    "            \n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "        \n",
    "        MCC = (tp*tn - fp*fn)/np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "        print(\"{} MCC: {}\".format(name, MCC))\n",
    "        confused_dict[name] = [tn, fp, fn, tp]\n",
    "        \n",
    "        \n",
    "    return confused_dict\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "confused_dict = prediction_results(models_to_try, X_train, X_test, y_train, y_test)\n",
    "\n",
    "def dict_to_output(confused_dict):\n",
    "    for key, value in confused_dict.items():\n",
    "        print(\"{} Confusion Matrix\".format(key))\n",
    "        print(pd.DataFrame({\"True Y=1\":[value[3],value[2]],\"True Y=0\":[value[1],value[0]]},index=[\"Guess Y=1\",\"Guess Y=0\"]))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "dict_to_output(confused_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
