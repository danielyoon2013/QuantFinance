{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changes from default:\n",
    "\n",
    "## Effect of changes:\n",
    "N/A\n",
    "\n",
    "## 0. Overview\n",
    "\n",
    "\"Quantifying unstructured and noisy textual content is complex and involves numerous methodological issues related to the preprocessing of the data and the optimization of the algorithm used to quantify textual content. The number of **text preprocessing** that can be implemented is numerous (**lowercase**, **stemming**, **lemma-tization**, part-of-speech tagging, **stopwords removal**, **punctuation removal**, etc.) and it is not easy to identify which transformation increases (decreases) the accuracy of the classiﬁcation. The same is true for the choice of the algorithm: the large number of algorithms (Naive Bayes, SVM, logistic regression, random forest, multilayer perceptron, etc.) and the even greater number of hyperparameters for each algorithm lead to an immense number of combinations.Furthermore, the answers relative to those methodological issues strongly depend on the type of data used (informal or formal content, short or long text), on the size of the dataset (few hundreds or mil-lions of documents), on the availability of pre-classiﬁed messages (supervised or unsupervised learning), and on the type of documents (domain-speciﬁc or generic documents). While there is no one-ﬁts-all solution, we nonetheless believe that some guidance and tips can help researchers to avoid common mistakes.\"\n",
    "\n",
    "Renault, Thomas. (2020). Sentiment analysis and machine learning in finance: a comparison of methods and models on one million messages. Digital Finance. 2. 10.1007/s42521-019-00014-x. pp. 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import nltk\n",
    "#nltk.download('all-corpora')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import word_tokenize, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load data\n",
    "Current status: Default (~5000 tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this in testing.\n",
    "data = pd.read_csv('stock_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Clean and Lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_lower(text):\n",
    "    '''\n",
    "    Input:\n",
    "        A string of messy data\n",
    "    Output:\n",
    "        A string of clean lowercase data\n",
    "    \n",
    "    1) Replacing everything that isn't a letter with a space (special characters, numbers)\n",
    "    2) Sending all text to lowercase\n",
    "    '''\n",
    "    clean_text = re.sub('[^a-zA-Z]',\" \", text)\n",
    "    clean_text = clean_text.lower()\n",
    "    return clean_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Tokenize\n",
    "Current status: Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(text):\n",
    "    '''\n",
    "    Input:\n",
    "        A string of cleaned data\n",
    "    Output:\n",
    "        A list of the words in the string\n",
    "        \n",
    "    Will split the string into tokens based on what nltk package thinks is best.\n",
    "    Not sure how different it is compared to string.split(' ')\n",
    "    \n",
    "    There is also a TwitterTokenizer, maybe something to look into if we want to deal with \n",
    "        emojiis instead of replacing them with spaces.\n",
    "    '''\n",
    "    return word_tokenize(text, language='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Lemmatize and Stopword removal\n",
    "Current status: Lemmatization on\n",
    "\n",
    "Stopword removal on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma_and_stopwords(text, remove_stop_words=False):\n",
    "    '''\n",
    "    Input:\n",
    "        List of words that have been cleaned and tokenized\n",
    "    Output:\n",
    "        A string that is supposed to represent the meaning of the original sentence, as reduced as possible\n",
    "        \n",
    "        \n",
    "    From from https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "    \"\n",
    "        Lemmatization is the process of converting a word to its base form. The difference \n",
    "            between stemming and lemmatization is, lemmatization considers the context and \n",
    "            converts the word to its meaningful base form, whereas stemming just removes the \n",
    "            last few characters, often leading to incorrect meanings and spelling errors.\n",
    "\n",
    "        For example, lemmatization would correctly identify the base form of ‘caring’ to ‘care’, \n",
    "            whereas, stemming would cutoff the ‘ing’ part and convert it to car.\n",
    "\n",
    "        ‘Caring’ -> Lemmatization -> ‘Care’\n",
    "        ‘Caring’ -> Stemming -> ‘Car’\n",
    "    \"\n",
    "    \n",
    "    \n",
    "    Since the original paper also mentioned stop words, I made sure the word is not a stop word\n",
    "    before trying to convert it. I ran it with and without the stop word and I didn't notice much difference\n",
    "    but maybe it's something to fiddle with later.\n",
    "    \n",
    "    \n",
    "    EDIT 1: The paper also mentioned 'part-of-speech' tagging, which seems to attach context to each word to help\n",
    "    convert its meaning properly. Since this seemed more complicated, and this isn't a text processing project,\n",
    "    I didn't do it. \n",
    "    However there is an entire coded example (example 3) here: https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n",
    "    so it might be quick to implement and see if it improves our classification scores after\n",
    "    \n",
    "    EDIT 2:\n",
    "    About stop words, from the paper: \n",
    "        We also ﬁnd that removing stopwords using the NLTK stopwords corpus signiﬁcantly decreases the accuracy \n",
    "        of the classiﬁcation. We believe that this result is due to the fact that the stopwords corpus from NLTK includes \n",
    "        words that could be very useful for sentiment analysis in ﬁnance such as “up”, “down”, “below” or “above”. Thus, \n",
    "        researchers should not use the standard NLTK list and should consider a more restrictive list of stopwords for \n",
    "        sentiment analysis (“a”, “an”, “the”...). This result is consistent with Saif et al (2014) who show that Naive \n",
    "        Bayes classiﬁers are more sensitive to stopword removal and that using pre-existing lists of stopwords negatively \n",
    "        impacts the performance of sentiment classiﬁcation for short-messages posted on social media.\n",
    "    '''\n",
    "    if remove_stop_words:\n",
    "        list_of_words = [lemmatizer.lemmatize(word) for word in text if(word) not in stop_words_set]\n",
    "    else:\n",
    "        list_of_words = [lemmatizer.lemmatize(word) for word in text]\n",
    "        \n",
    "    string_of_words = \" \".join(list_of_words)\n",
    "    return string_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_all_data(data, remove_stop_words=False):\n",
    "    '''\n",
    "    Input:\n",
    "        The column of the dataframe that contains text\n",
    "    Output:\n",
    "        A list of strings that has been through:\n",
    "            clean_data = clean_and_lower(data)\n",
    "            token_data = tokenize_data(clean_data)\n",
    "            lemma_data = lemma_and_stopwords(token_data)\n",
    "    '''\n",
    "    list_of_nice_strings = []\n",
    "    for i in data:\n",
    "        clean_string = clean_and_lower(i)\n",
    "        token_list = tokenize_data(clean_string)\n",
    "        lemma_string = lemma_and_stopwords(token_list, remove_stop_words=remove_stop_words)\n",
    "        list_of_nice_strings.append(lemma_string)\n",
    "        \n",
    "    return list_of_nice_strings\n",
    "\n",
    "# Note: Just eyeballing it, it looks like some words that are useful are being thrown out\n",
    "# I.e in fourth row (index 3) it goes from \"MNTA Over 12\" --> \"mnta\". It seems like that is a bullish tweet, but something\n",
    "# is throwing away the \"over\".\n",
    "# EDIT: I just tried without \"stop words\" and it recovered the \"Over\"... Best to try both\n",
    "\n",
    "clean_data = prep_all_data(data['Text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Vectorization\n",
    "Current status:\n",
    "\n",
    "Vectorization: CountVectorizer\n",
    "\n",
    "n-grams: 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_num(clean_data):\n",
    "    '''\n",
    "    Input: \n",
    "        cleaned list of strings\n",
    "    Output:\n",
    "        Numerical vector representation [0's and 1's]\n",
    "    \n",
    "    From: https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n",
    "    More info: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "    \n",
    "    Text Analysis is a major application field for machine learning algorithms. However,\n",
    "        the raw data, a sequence of symbols cannot be fed directly to the algorithms \n",
    "        themselves as most of them expect numerical feature vectors with a fixed size \n",
    "        rather than the raw text documents with variable length.\n",
    "    ...       \n",
    "    We call vectorization the general process of turning a collection of text documents into numerical \n",
    "        feature vectors. This specific strategy (tokenization, counting and normalization) is called \n",
    "        the Bag of Words or “Bag of n-grams” representation. Documents are described by word occurrences \n",
    "        while completely ignoring the relative position information of the words in the document.\n",
    "    '''\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    \n",
    "    # Like usual we may need to cross validate this to determine the optimal represenation\n",
    "    # Cross validation link: https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py\n",
    "    # max_features = ?\n",
    "    # ngram_range = ?\n",
    "    # fit_transform: Learn the vocabulary dictionary and return document-term matrix.\n",
    "    # EDIT: He has results in his paper about choosing good parameters for ngrams\n",
    "\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 2), min_df=3)\n",
    "    document_term_matrix = vectorizer.fit_transform(clean_data).toarray()\n",
    "    return document_term_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dimension reduction\n",
    "Current status:\n",
    "\n",
    "No dimension reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# None by default.\n",
    "# When we add this, we'll do it off the X matrix here.\n",
    "\n",
    "X = pd.DataFrame(clean_data)\n",
    "# X = text_to_num(clean_data)\n",
    "# for word2vec model, we need to transfer X later\n",
    "y = data[\"Sentiment\"]\n",
    "y = y.replace(-1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Word2Vec Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_transform_features(clean_data2, model, size, same_origin=False):\n",
    "    # clean_data2, list of list, in first layer, one list is a sentence, in the second layer, each list contains words it has\n",
    "    # model, a gensim word2vec model, can take words as input\n",
    "    # size, the representing vector length\n",
    "    # it is possible that the words in the train set doesn't exist in the test set\n",
    "    if same_origin:\n",
    "        print('model comes from the data')\n",
    "    else:\n",
    "        print('model is a pretrained model')\n",
    "\n",
    "    vec_list = []\n",
    "    for sentence in clean_data2:\n",
    "        curr_sentence_vecs = []\n",
    "        in_count = 0\n",
    "        notin_count = 0\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vec = model[word].reshape(-1,1)\n",
    "                curr_sentence_vecs.append(vec)\n",
    "                in_count += 1\n",
    "            except KeyError:\n",
    "                # print(f\"Not in dictionary: {word}\")\n",
    "                notin_count += 1\n",
    "        if len(curr_sentence_vecs)>0:\n",
    "            vec_list.append(curr_sentence_vecs)\n",
    "        else:\n",
    "            vec_list.append([np.zeros(size).reshape(-1,1)])\n",
    "    vec_list2 = [np.concatenate(x, axis=1) for x in vec_list] #combine vectors to a matrix\n",
    "    vec_list3 = [np.sum(x, axis=1) for x in vec_list2] # sum the vectors as a vector to represent the sentence\n",
    "    vec_list4 = [x.reshape(1,-1) for x in vec_list3]\n",
    "    feature_mat = np.concatenate(vec_list4, axis=0)\n",
    "    \n",
    "    return feature_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training/Testing split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 300\n",
    "#simple_model = gensim.models.Word2Vec(X_train_l2,size=size)\n",
    "model = api.load(\"word2vec-google-news-300\")  # download the model and return as object ready for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape: (5791, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print(\"X Shape: {}\".format(X.shape))\n",
    "# Should we do PCA on this? X is (5791, 8330)....\n",
    "# from sklearn.decomposition import PCA\n",
    "# n_components=250\n",
    "# pca = PCA(n_components=n_components) \n",
    "# X_reduced = pca.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2021)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_word =X_train \n",
    "X_test_word = X_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer the dataframe of sentences back to list in order to used in word2vec model\n",
    "def df2list(X):\n",
    "    result = [X.iloc[i,0] for i in range(len(X))]\n",
    "    return result\n",
    "\n",
    "X_train_l = df2list(X_train)\n",
    "X_test_l = df2list(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2list(clean_data):\n",
    "    clean_data2 = [x.split(' ') for x in clean_data]\n",
    "    return clean_data2\n",
    "\n",
    "X_train_l2 = sent2list(X_train_l)\n",
    "X_test_l2 = sent2list(X_test_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "# info = api.info()  # show info about available models/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model comes from the data\n",
      "model comes from the data\n"
     ]
    }
   ],
   "source": [
    "size=300\n",
    "X_train = word2vec_transform_features(X_train_l2, model, size, same_origin=True)\n",
    "X_test = word2vec_transform_features(X_test_l2, model, size, same_origin=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Model list definition:\n",
    "Current status: Multinomial NB, Logistic Regression, SVM, Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "models_to_try = []\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "models_to_try.append((\"GaussianNB\",GaussianNB()))\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "# Fitting gaussing naive bayes (like in class) instead\n",
    "# models_to_try.append((\"GaussianNB\", GaussianNB()))\n",
    "\n",
    "# Logistic Regression\n",
    "models_to_try.append((\"LogisticReg\", LogisticRegression()))\n",
    "\n",
    "# Support Vector Machine\n",
    "models_to_try.append((\"SVC\", SVC()))\n",
    "# models_to_try.append((\"SVC\", SVC(probability=True))) # Need this to use .predict_proba()\n",
    "\n",
    "# Random Forest\n",
    "models_to_try.append((\"RandomForest\", RandomForestClassifier()))\n",
    "\n",
    "# Multilayer Perceptron \n",
    "# Not going to fit a multilayer perceptron, here's XGB instead\n",
    "#models_to_try.append((\"XGB\", XGBClassifier()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB Accuracy: 0.6065573770491803\n",
      "GaussianNB AUC: 0.6191755928385867\n",
      "GaussianNB MCC: 0.15374202404461323\n",
      "LogisticReg Accuracy: 0.7385677308024159\n",
      "LogisticReg AUC: 0.7878322519547998\n",
      "LogisticReg MCC: 0.421657678879287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy: 0.7575496117342536\n",
      "SVC MCC: 0.4627771252460948\n",
      "RandomForest Accuracy: 0.7109577221742882\n",
      "RandomForest AUC: 0.765152096850565\n",
      "RandomForest MCC: 0.34196785653215794\n",
      "GaussianNB Confusion Matrix\n",
      "           True Y=1  True Y=0\n",
      "Guess Y=1       505       229\n",
      "Guess Y=0       227       198\n",
      "\n",
      "\n",
      "LogisticReg Confusion Matrix\n",
      "           True Y=1  True Y=0\n",
      "Guess Y=1       617       188\n",
      "Guess Y=0       115       239\n",
      "\n",
      "\n",
      "SVC Confusion Matrix\n",
      "           True Y=1  True Y=0\n",
      "Guess Y=1       634       183\n",
      "Guess Y=0        98       244\n",
      "\n",
      "\n",
      "RandomForest Confusion Matrix\n",
      "           True Y=1  True Y=0\n",
      "Guess Y=1       664       267\n",
      "Guess Y=0        68       160\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import  roc_auc_score\n",
    "def prediction_results(models_to_try, X_train, X_test, y_train, y_test, X_train_word, X_test_word):\n",
    "    '''\n",
    "    Input:\n",
    "        models_to_try: a list of tuples (\"Name as a string\", object)\n",
    "        Data as usual\n",
    "    Ouput:\n",
    "        Prints the accuracy and returns a dictionary to create a confusion matrix\n",
    "    \n",
    "    '''\n",
    "    confused_dict=dict()\n",
    "    sample_incorrect = dict()\n",
    "    for name, classifier in models_to_try:\n",
    "\n",
    "        classifier.fit(X_train, y_train)\n",
    "            \n",
    "        preds = classifier.predict(X_test)\n",
    "        \n",
    "        preds_train = classifier.predict(X_train)\n",
    "        X_trained_incorrect = X_train_word[preds_train!=y_train.values]\n",
    "        X_test_incorrect = X_test_word[preds!=y_test.values]\n",
    "        \n",
    "        sample_incorrect[name] = (X_trained_incorrect, X_test_incorrect)\n",
    "        \n",
    "        print(\"{} Accuracy: {}\".format(name, accuracy_score(y_test, preds)))\n",
    "        \n",
    "        if name != \"SVC\":\n",
    "            probs = classifier.predict_proba(X_test)[:, 1]\n",
    "            print(\"{} AUC: {}\".format(name, roc_auc_score(y_test, probs)))\n",
    "            \n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n",
    "        \n",
    "        MCC = (tp*tn - fp*fn)/np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "        print(\"{} MCC: {}\".format(name, MCC))\n",
    "        confused_dict[name] = [tn, fp, fn, tp]\n",
    "        \n",
    "        \n",
    "    return confused_dict, sample_incorrect\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "confused_dict,sample_incorrect = prediction_results(models_to_try, X_train, X_test, y_train, y_test, X_train_word, X_test_word)\n",
    "\n",
    "def dict_to_output(confused_dict):\n",
    "    for key, value in confused_dict.items():\n",
    "        print(\"{} Confusion Matrix\".format(key))\n",
    "        print(pd.DataFrame({\"True Y=1\":[value[3],value[2]],\"True Y=0\":[value[1],value[0]]},index=[\"Guess Y=1\",\"Guess Y=0\"]))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "dict_to_output(confused_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aap with billion in cash that is crazy\n",
      "jpm not looking pretty here\n",
      "sensex end point lower nifty settle at a market take hit after a day s breather amid coronavi http t co dzp suya v\n",
      "goog igv google software firing on all cylinder compq ndx\n",
      "selling bvsn here from stop b e even\n",
      "glad that the easy money trade is over every joker thought that making money in the market is synonym with getting long aap\n",
      "emn increasing selling volume next support is the ascending trend line\n",
      "aap want to buy more around\n",
      "af watching spx high and relative strength weakness af at ma\n",
      "spy ncle ben needed to spice up thing a little aap people were bored wake up now\n",
      "state government scramble for fund a coronavirus take toll on coffer http t co kztfrvxhci\n",
      "nvda nvidia show off phoenix reference phone at mwc\n",
      "a the coronavirus pandemic intensifies adherent of the financial independence retire early movement are doublin http t co klzyro r\n",
      "aap about to go red amazing how many time that pattern can repeat over and over\n",
      "jpmorgan chase chief executive james dimon returned to work this week a month after undergoing emergency heart sur http t co n vm lyph\n",
      "mtg between yesterday session and today s pm almost the entire float ha been traded lol\n",
      "grocery tycoon kishore biyani s battle to keep his company afloat http t co vdzbhnh o\n",
      "user prop trader user you have the january affect of new inflow and new money wasn t going into aap clearly\n",
      "jcp piper out with target i think it s should be\n",
      "disney cancel the clone war plan tv show of it own exploring star war character and storyline dis twx\n",
      "aap now glass ceiling rejecting both and now\n",
      "pay ncertainty is never a good thing\n",
      "aap are rumour news idontthinkso we know there are not many seller below\n",
      "the economy ha almost certainly entered a recession affecting most of the world with a severity unmatched by anyt http t co j hpwxg\n",
      "aap like i said yesterday appe is a broken stock and tc is not the right ceo for the company still shot\n",
      "aise nd of bvsn to\n",
      "ssys exuberance is a stock generating million in operational cash flow for mo trading at billion or u could say a top\n",
      "goog new od\n",
      "aap wall st most hated stock currently careful short anything short of disaster will be viewed positively bit risky long mainevent\n",
      "af min second shot at the ma crossing\n",
      "failure to complete the deal would be a blow to bp which already ha the highest debt level among the major oil c http t co w nde buxe\n",
      "global oil demand will drop by million barrel a day in with the sharpest contraction coming in april ami http t co uqvtcl bqm\n",
      "official expect another surge a application processed for newly eligible gig economy worker self employed http t co fzzrxqmyl\n",
      "china s restriction strand face mask coronavirus test kit and other critical product bound for the u s state http t co jadokqbjj\n",
      "rt nicktimiraos business not government have a huge say in how economy reopen after the pandemic recedes http t co wfrnrsokn\n",
      "rt lizrhoffman this is an absolutely wild chart the fed grew it balance sheet by almost trillion in the last two week twice a f\n",
      "fio barely red this stock due for one of it s famous rip higher keep an eye i bought some yesterday\n",
      "so glad i sold pay around i wa tempted to buy back in now won t touch it till all dust settle it s in norman s land\n",
      "bac wait for it to setup again ooks like it will retest for sure\n",
      "med to volume picking up\n",
      "today is the very important day we haven t closed green for day in a row since sept very bullish short term if this happens aap\n",
      "ba for a flush down\n",
      "blog post\n",
      "shot setup aap goog bby\n",
      "f solar market to ise percent in deutsche bank predicts\n",
      "cee want then i think it s time is up imo\n",
      "wall street resume trading after slide trigger halt http t co ivybmkqag\n",
      "aap someone catch a leak of some news at or what\n",
      "pphm is going down like a ship\n",
      "now watching ba\n",
      "polk expects s new vehicle egistrations to each million polk f gm i think this is low\n",
      "shd eddie ampert is a trick pony\n",
      "spw the dip that refreshes gartley target on month daily a b c d\n",
      "estimate to move lower for safm after eps and a trend line break likely today with support at\n",
      "with a single tweet thursday president trump accomplished what decade of war coup terrorist attack and opec http t co nysxltp uu\n",
      "amzn alert update it went hard against u but is playing out now i would consider adding on or getting in here\n",
      "ost m my position hurt so bad today butt it just slide back to last wednesday maintain target\n",
      "shot mtg\n",
      "gae xi pharmaceutical xii announces pcoming presentation\n",
      "vxy aap bottom look in add fa here fed short more vxy acct vacuum cleaner gdp is od sandy news we have relief fund in now\n",
      "user user aap daily three black crow and ema teaming up\n",
      "ssys add over million in market cap today\n",
      "split adjusted kansa city ailroad wa cent in and just in today an all time high of short aap goog hmmm\n",
      "owered my price target for gpn to\n",
      "swy pretty lame selling swy because wmt sold horse meat in europe th spy\n",
      "published znga estimate on estimize eps and m ev compared to wall street s consensus\n",
      "new industry data provide the first hard look at how many american are struggling to make rent during the coronavi http t co jnd fmoz\n",
      "swhc obama biden to speak tomorrow on gun control legislation\n",
      "bby headed back to on over saturated cell phone market that goog will own a next mobile microsoft o\n",
      "aap weekly si finally back under st time since autumn by lower trend line williams\n",
      "vxy added some aap mar bull straddle for eod squeeze\n",
      "eyeing adbe a a possible short set up would like to see a little more downside volume first\n",
      "ok aap bear turn off your user they are aboutto explain why aap is going to break out soon it will have technical so you ignore\n",
      "bac ot of big seller on new od again expect a big sell of eod\n",
      "aa i don t like the spin on operational cash flow\n",
      "rbi ha been proactively monitoring coronavirus situation very closely governor shaktikanta da amid covid crisis\n",
      "sem see this going down\n",
      "aap when the market hate ow p e high eps no debt and hge profit something is wrong imho\n",
      "ht bearish\n",
      "ebay paypal revenue is increasing by providing credit you want to hold a credit provider or a payment processor\n",
      "vxy added nd tranche short dead cat bounce done ba bull\n",
      "amzn bearish set up ahead of earnings tct member i will be writing a note this evening re trade\n",
      "we explain in the jan th aapl video how not to be gaffed by the operator fish hook aapl might be great co but broken sto\n",
      "news on n\n",
      "paradoxically the fed s success hold the key for further rally in the price of gold http t co lnhqomqiac\n",
      "watch nx under\n",
      "sold partial skx it is in a bear market you make most of your money you just don t know it then aising\n",
      "coh no\n",
      "user aap is too far highly unlikely cross ever again will c more than likely now is next level watch\n",
      "for the past year f ha booked an avg return of from jan to mar option expiration f ha posted negative return for year\n",
      "sensex nifty end mixed despite rbi s steep reo rate cut http t co vet pazo d\n",
      "amzn how low will it go\n",
      "an is at least gon na sink to\n",
      "vitc long higher low with high stuck at sma zone c if it clear print a candle\n",
      "msi shot will see the price around very soon month becefull have a nice day\n",
      "the aap drop will stop only when wall street and seller stop fearing the margin drop take a look\n",
      "aap take your profit sunshine and repeat\n",
      "bank issue order to employee over branch overcrowding report http t co q sd deydg\n",
      "super shot user please help me get this viral i mean what a putt under pressure goog\n",
      "app on a tear great short squeeze play\n",
      "sfd here it come\n",
      "goog may roll over here\n",
      "u s stock are staging a remarkable two day rally but many analyst are calling the recent run a rally within a b http t co dr j oyo\n",
      "user an error on morn data it not updated share outstanding equal million not million mkt cap bil not bil\n",
      "india is expected to post sharp turnaround in say rbi governor quoting imf projection\n",
      "g going to\n",
      "dov nov eport feb early release earnings next month ha made major stride cab ta\n",
      "save continues run all time high within sight our budget airline analysis from fall\n",
      "aap please fasten yor seatbelt and enjoy the ride\n",
      "swy wow doing a number on the bear this week scaled out of a little today\n",
      "ebay ebaynow wa dead on arrival and the company itself is leveraged a lot\n",
      "user me too but nfx ha been kicking my a\n",
      "amazon flipkart operation being disrupted amid lockdown report http t co lk rpczv\n",
      "pphm ike i said jump off the ship my foot are wet\n",
      "aap volume fade fast\n",
      "too early to short into this move stock market need a few day to settle down patience coh bwd dt aap pay\n",
      "mtg holding small short po from for swing\n",
      "indian bank are safe and there is no need to resort to panic withdrawal rbi governor shaktikanta da\n",
      "the implosion of a starbucks wannabe in china show how expensive it can be to do reliable due diligence on faraway http t co lqqoc vocw\n",
      "faz about to explode get eady bac just broke under the day\n",
      "aap mac manufactured un sa said obama ower profit margin\n",
      "won t believe aap uptrend is back until it cross above ma\n",
      "green daily triangle on mnk net profit\n",
      "heard on the street don t be fooled by improving chinese export http t co q v bc wl\n",
      "aap so according to this article apple ha some work to do definitely sound like they are a step ahead\n",
      "nfx anyone noticing the weak candle support in the last hour\n",
      "aap new yr low today for apple could be next\n",
      "is it just me or doe that look like a double top on hd\n",
      "aap bearish macd crossover\n",
      "for the past year goog ha posted a best return worst return during the jan to mar option expiration time frame\n",
      "oh cee\n",
      "nw ong trailing stop from prior stop of and\n",
      "pi q q operational cash flow for mo million decline v mo year ago\n",
      "goog in the put for aap is crashing\n",
      "government to borrow more aggressively in april september than expected http t co cbv yi djb\n",
      "nfx what a brutal short squeeze i hope to see the same on bcn znga vvs amd gtat and so on\n",
      "awk en fuego part\n",
      "aap here come the european maybe they will save u\n",
      "i don t like the technical on omx but the fundamental look like they could turn around with a few good quarter\n",
      "aig american international group option trader bet on down move by next friday\n",
      "aap gap fill and go hope so\n",
      "wmt if i had enough cash on hand i d be shorting this right about now\n",
      "bac what happens next\n",
      "market revise trading rule hour circuit breaker a volatility surge key fact http t co jqnhn poa\n",
      "watch xf jpm bac od closely\n",
      "day npa norm not applicable on moratorium granted on existing loan by bank say rbi governor\n",
      "equity esearch morning post at my home page ssys ddd\n",
      "will watch the close carefully then decide whether to ong or shot aap below shot\n",
      "sk to the gave\n",
      "rt pauljdavies i mean just jaw dropping gt gt a job loss ricochet across the u s europe is conducting an unprecedented experiment in n\n",
      "heard on the street the plunging value of the portfolio company in softbank s billion vision fund account http t co j egeajzr\n",
      "aap heading for the gap\n",
      "more bad aap news they might ony se b not projected b se se se\n",
      "cee this is about to tip over the edge and go over the fall target\n",
      "trade idea buy esc target cut below\n",
      "gmx close to the fence\n",
      "ffiv looking for a quick move to support at especially if spy lead the way down\n",
      "co had this on the list for break of and risk wa too much so no buy man i missed a good one low volume deterrent caused the miss\n",
      "buying nfx put for tomorrow avg\n",
      "r lakh crore package for migrant worker poor finance minister nirmala sitharaman http t co ucczezkyi\n",
      "nvda ooks bullish but it must stay above on a closing basis monthly eps say no\n",
      "user buy on a dip is dip enough aap\n",
      "g ask is being pummeled manager can not miss this and afraid they are missing the train\n",
      "news on apo\n",
      "user aap finishing wave of decline i think an error in your calc is incl einhorn s pump dump\n",
      "wfc sb is culprit for weakness bowe put target\n",
      "ed weekly triangle on fnfg net profit\n",
      "aap min short won now longs have fully added in look for a pop here with market shakeout reset\n",
      "the surging popularity also ha given rise to a new form of online harassment known a zoombombing where online t http t co ixdco kug\n",
      "affy little downside huge upside still holding from thursday at cost basis of from trimming into thursday s rally\n",
      "indian startup oyo hotel amp home is planning to move some employee off it payroll and onto the roll of it bigg http t co vscd rhe\n",
      "bac new od\n",
      "aap broke trend\n",
      "bid put were active yesterday going out to jan\n",
      "american apparel app present at\n",
      "in past day more money came to yes bank compared to withdrawal a only one third of customer withdrew r http t co siayn tze\n",
      "watch cat under could flush all the way down to\n",
      "aap min evening star reversal with potential downtrend resistance\n",
      "ea if price doesn t hold above sma then is the next support\n",
      "aap range is to can go towards to lower end\n",
      "oh lord dks\n",
      "consumer inflation at in february http t co lojpnfj r\n",
      "amzn headed to lower weekly channel support at approx\n",
      "aap min i m in otus position trying to emulate user\n",
      "closed all my any call today looking to re enter on a pull back\n",
      "gmx bingo\n",
      "wa supposed to be the year everything came together for airbnb the coronavirus ha made that next to impossib http t co brrqvoraz\n",
      "user ddd broke out from the bear flag to the upside ots of short who do not understand momentum being caught stdy\n",
      "deck let squeeze short\n",
      "aap break of should see retest of low and lower fairly soon thereafter\n",
      "cat look to be bottoming but it also ha not moved much off it low with recent strength if it break to downside it good short\n",
      "wpi whoopie that s the stock nickname\n",
      "user getting excited about aap on the th bounce attempt is a bit silly\n",
      "bac is not going to lose not sure why these bozo s are holding her back here xf is off to the race\n",
      "user user goog how do you know goog is going to can you share that reasoning with u he is a george castanza\n",
      "h great risk v reward with stop under fib at\n",
      "user dtradingtrader i do have mio to expend and race up any offer for depo\n",
      "c should pop of this level stop under low if wrong\n",
      "calm before the storm the u s housing market wa set for a strong year before the coronavirus pandemic http t co q jrwc dr\n",
      "snts getting put at the open eally pleased that a decision came after hour allows for much better entry into position\n",
      "british airway in talk with union to suspend around staff report http t co djyn xamm\n",
      "vfc we are short this one again bull need to hold the support it is in now or the break could be significant\n",
      "aap now it is time to fill the gap to the up above\n",
      "new post short will make dime ongs will make dollar aap spx qqq p\n",
      "the sec should launch an investigation into citron this company eleased inaccurate information on ddd they only short sell to profit\n",
      "affy oversold etter is precaution not a death sentence\n",
      "market research firm idc and gartner both reported a large and surprising drop in first quarter pc sale on monday http t co g gnzn iw\n",
      "gtxi long will take early assuming no gap spy green detect above avg vol\n",
      "aap ow risk with a tight stop here however would like to see this one get some follow through today to better confirm\n",
      "wholesale inflation eas to in february from in previous month\n",
      "aap downgraded at jefferies to hold from buy jefferies said price target\n",
      "mhk i meant to say td buy setup nderway\n",
      "gold future rise to hover near r amid coronavirus fear http t co cztpvszatz\n",
      "instead of the classic m night shyamalan twist we got the recent m night shyamalan disappointment aap\n",
      "wow not good for bk t user sale for the nook were down over the holiday\n",
      "crude oil market set for record surplus amid coronavirus led demand slump goldman sachs http t co ziy kqur\n",
      "vmware vmw where here there best wish dealing with this complex h s\n",
      "answer to question from wall street journal reader about how the temporary suspension of required minimum distri http t co bxf natubb\n",
      "yhoo can it break this y channel and resistance mid term bearish until proven otherwise\n",
      "are we seeing a minor pullback for nfx or the beginning of a major correction this investor isn t about to test the water still bearish\n",
      "user ssys eached si crash is near\n",
      "bac wfc in the hole finally\n",
      "the new wightwatchers ad are more fun and light and with regular people more interesting but i still dont trust them wtw no position\n",
      "cien still not done going down\n",
      "user aap rookie are selling today mest loose otof being stubborn\n",
      "if aap revers early gain qqq will be in really big problem\n",
      "user aap divi announce came on third monday of march the th will it come on march th this year\n",
      "oc osing mkt share to jds t now and looking for much lower price will miss earnings too\n",
      "some retail chain fight for survival amid coronavirus http t co x gwibmvb\n",
      "user bac covered all my short right here wa a good move maybe next week after dividend will short\n",
      "znga there are a lot of support here smas i just bot few share at falling on low volume\n",
      "retail inflation could slow further going ahead expert http t co s hzhx u q\n",
      "pki been stalking this one for a while may be giving u a low risk entry monday\n",
      "outbreak how coronavirus swept through jpmorgan s trading floor http t co dumydnh qy\n",
      "user aap hit hedge fund margin leverage highest level since the question is how many hedge fund blew up today\n",
      "ca continuing to work well glad i bought a decent size block probably cutting out of this one soon lol\n",
      "ffiv another cs falling through lower trend support today seeing a trend here\n",
      "aap need to hold here or could see lower price earnings within couple wks\n",
      "o orillard option trader buy k put betting on a down move a bad earnings omen\n",
      "cat caterpillar machine etail sale drop accelerates ed by asia\n",
      "fan just keep on giving\n",
      "bac sell off of the day begin\n",
      "rt spencerjakab please ignore the i saw the coronavirus coming here s what s next stuff and read my latest for wsjheard instead htt\n",
      "cook should inquire a to how many indictment goldman ha faced every time he is asked for number walk off the stage g aapl\n",
      "sensex nifty likely to open lower after a day s breather http t co xetjtnwtyq\n",
      "growth may slip into negative in q ex rbi governor c rangarajan on lockdown http t co fyfjombgsm\n",
      "pan went hard in the paint this week tsa on a comback too i put my allowance money there\n",
      "travel hospitality to take time to recover post lockdown expert http t co ilkyxeesnb\n",
      "nfx short moved target for reversal up not in yet s and ma\n",
      "not even drugstore chain can tell investor what the future look like investor of all sort should take notice http t co lnibof atr\n",
      "we haven t really seen market reflect the full extent of the damage that coronavirus is having to corporate profi http t co jtm mpi jr\n",
      "happy easter shot setup c isg cf an nkd ow moh mtz bi\n",
      "isn t aap getting unjustly punished for having too much cash good problem to have no i still love my iphone happily trapped in ecosys\n",
      "longtail alpha fund capitalized on the market reaction to the new coronavirus pandemic http t co hjaxfamwhn\n",
      "a in a clear downtrend with no sign of relief\n",
      "rt minzengwsj wsj boeing is expected to begin offering early retirement and buyout package to it workforce a the plane maker come to\n",
      "jcp still cracking\n",
      "znga is about to take off watch out short\n",
      "cog rolling over here think it will lose most of this move today\n",
      "athx macd cross down\n",
      "icici bank to invest up to r crore in crisis hit yes bank http t co alpdgelncx\n",
      "c you can say whatever you like about the mo but thats a mo long head n shldrs folk on yr\n",
      "exclusive a top occidental petroleum executive ha been forced out of the company a it deal with the repercussio http t co yfkn foqg\n",
      "mhp kid don t do steroid\n",
      "nvda remains trapped between the sma and the sma slow sto trying to recover cross over\n",
      "g is current support fib evel fyi\n",
      "today s watchlist for shot stock mtg itmn anac oww dmnd\n",
      "nfx mar c mar th expiry tos show it firing at the bid x but sorry folk bot\n",
      "didn t plan on holding these msft apr call overnight again but when do thing ever go a planned\n",
      "user aap will finish green a i predicted i agree\n",
      "milwaukee s cat to lay off employee in so mke branch\n",
      "kicker on my watchlist xide tit soq pnk cpw bpz aj trade method or method see prev post\n",
      "aap dumping into the close still see this going sub maybe at or after earnings report in a wk or two gy town\n",
      "here is the amzn with the down target magenta at\n",
      "aap breaking support on heavy volume sharing a few insight video\n",
      "nvda to answer your question entry add wa the red candle at am today ove flag\n",
      "user still holding those put mcp\n",
      "aap flash dump in progress done be patient or press escspe\n",
      "vocs example on min using cose below ema not low for your stop not traded but good for\n",
      "ed weekly triangle on hek net profit\n",
      "shd sears look good on break out this morning shd\n",
      "vng mil share short plus long buying massive vng short squeeze just when you thought story wa dead avicher kill short thesis\n",
      "user i think the same aapl iphone lost the wow factor\n",
      "amazon to add more job amid coronavirus pandemic http t co niwbvxfemi\n",
      "heard on the street norwegian air could succeed in it last ditch effort to survive the coronavirus crisis but wi http t co murlpuzk\n",
      "jeff eeves and charles sizemore discus inkedin earnings on the slant nkd fb amzn mww\n",
      "svm ha come down approx sv price may swing straddle option h svc si miner to gain exposure of the sv dsv to hdge\n",
      "nkd just delivered the knockout punch to the short still fighting this beast\n",
      "user aap going to drop hard when rumor is flushed out no rumour hit a buy target dma\n",
      "if vhc break the support level at then it could bring area no reason to buy yet\n",
      "cpb sitting at a week high i am short but bracketed with being my stop and being my trigger not looking good right now\n",
      "aap breaking important level here\n",
      "user bac no longer a problem\n",
      "aap almost every long will want to goo out for the week end nobody wnat to be caught for month\n",
      "aap i shorted after e and win but now it s oveeacting stock will be around by eod tomorrow\n",
      "user not feeling heop good luck\n",
      "so far it look like wynn failed at ma\n",
      "fio crushed\n",
      "shot ao\n",
      "gold price spike a u joblessness data lift safe haven demand http t co m nswhy x\n",
      "massive sell off aap geat\n",
      "ebay management should be fired on that comment alone what were you waiting for knucklehead\n",
      "sensex open point lower at nifty at a global stock tumble amid more lockdown on coronaviru http t co jz losqz\n"
     ]
    }
   ],
   "source": [
    "for i in range(sample_incorrect['LogisticReg'][1].shape[0]):\n",
    "    print(sample_incorrect['LogisticReg'][1].iloc[i,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
